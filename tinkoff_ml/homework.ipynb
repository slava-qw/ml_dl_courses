{"cells":[{"cell_type":"markdown","id":"5372eb6b","metadata":{"id":"5372eb6b"},"source":["# Домашняя работа по регуляризации и оптимизации\n","\n","Ниже приводится корпус данных с двумя метками: 1 и -1. К данным применяется линейная модель классификации:\n","\n","$f(x, \\theta) = x_1 \\theta_1 + x_2 \\theta_2 + \\theta_3.$\n","\n","Предлагается подобрать параметры $\\theta$ минимизируя следующую функцию ошибки:\n","\n","$\\mathcal{L}(\\theta) = 0.1 (\\theta_1^2 + \\theta_2^2) + \\frac{1}{N}\\sum\\limits_{i=1}^N \\max(0, 1 - y_i f(x_i, \\theta)).$\n","\n","Для оптимизации предлагается использовать метод градиентного спуска с 1000 шагами размера $0.1$ из начальной точки $(1, 1, 0)$."]},{"cell_type":"markdown","source":["Useful liniks: https://en.wikipedia.org/wiki/Hinge_loss, https://en.m.wikipedia.org/wiki/Ridge_regression, https://en.wikipedia.org/wiki/Support_vector_machine, https://habr.com/ru/company/ods/blog/484148/\n","\n","Для удобства решения загоним bias в матрицу данных, то есть для одного семпла:\n","$$\n","x_1 \\theta_1 + x_2 \\theta_2 + \\theta_3 =\n","\\begin{pmatrix}\n","x_1 & x_2 & 1\\\\\n","\\end{pmatrix} \\cdot\n","\\begin{pmatrix}\n","\\theta_1 \\\\\n","\\theta_2 \\\\\n","\\theta_3\n","\\end{pmatrix} = \\langle \\theta, x_{new} \\rangle = g(x_{new}, \\theta)\n","$$\n","\n","Где $\\theta = (\\theta_1 \\ \\theta_2 \\ \\theta_3)^T, x_{new} = (x_1 \\ x_2 \\ 1)^T$.\n","\n","Для градиентного спуска необходимо найти градиент функции ошибок, но сначала преобразуем её к более красивому виду:\n","\n","$$\n","\\mathcal{L}(\\theta) = 0.1 ||\\theta||_2^2  -0.1 \\cdot \\theta_3^2+ \\frac{1}{5}\\sum\\limits_{i=1}^5 H_i(M_i) =\n","$$\n","\n","$$\n","= 0.1 \\langle \\theta, \\theta \\rangle  -0.1 \\cdot \\theta_3^2 + \\frac{1}{5}\\sum\\limits_{i=1}^5 H_i(M_i)\n","$$\n","\n","Где $M_i = y_i g(x_{new,i}, \\theta) = y_i \\langle x_{new,i}, \\theta \\rangle $ - отступ (margin) (тут в данном случае $i$ означает индекс семпла, а не значение координаты вектора, тем более, что $M_i, y_i$ - это числа), $H(M)$ - hinge loss.\n","\n","Как известно:\n","\n","$$\n","{\\frac  {\\partial H_i }{\\partial \\theta_{j}}}={\\begin{cases} - y_i \\cdot (x_{new, i})_j&{\\text{if }}M_i < 1\\\\\n","0, &{\\text{otherwise}}\\end{cases}} = -y_i \\cdot (x_{new,i})_j \\cdot [M_i < 1]\n","$$\n","\n","Поясню это словами: мы берем $i$ семпл (вектор признаков $x_{new,i}$ из нашей матрицы $X$) и для того чтобы найти производную по $j$ координате ($j = \\{1, 2, 3\\}$) смотрим на значение марджина $M_i$ и в зависимости от него получим, либо 0, либо значение целевой метки для данного семпла умноженную на отрицательное значение $j$ компоненты нашего вектора признаков $x_{new,i}$ .\n","\n","Обощая на все три координаты, получим:\n","\n","$$\n","\\nabla_{\\theta} H_i = -y_i \\cdot [M_i < 1] \\cdot x_{new,i}\n","$$\n","\n","Тут $i$ - это тоже номер семпла из всей матрицы объектов $X$.\n","\n","Таким образом для $i$ компоненты градиента:\n","\n","$$\n","(\\nabla_{\\theta} \\mathcal{L}(\\theta))_i =  0.2 \\cdot \\theta_i - 0.2 \\cdot \\theta_i \\delta_{i3} + \\frac{1}{5}\\sum\\limits_{j=1}^5\\partial_i H_j , \\ i = \\{1, 2, 3\\}\n","$$\n","\n","Где для записи использовались символ Кронекера и сокращение $\\partial_i H_j  \\equiv \\frac  {\\partial H_j }{\\partial \\theta_{i}} $. Если расписать этот градиент по координатам, то получим:\n","\n","$$\n","\\nabla_{\\theta} \\mathcal{L}(\\theta) =\n","\\begin{pmatrix}\n","0.2 \\cdot \\theta_1 + \\frac{1}{5}\\sum\\limits_{j=1}^5 \\frac  {\\partial H_j }{\\partial \\theta_{1}} \\\\\n","0.2 \\cdot \\theta_2 + \\frac{1}{5}\\sum\\limits_{j=1}^5 \\frac  {\\partial H_j }{\\partial \\theta_{2}} \\\\\n","\\frac{1}{5}\\sum\\limits_{j=1}^5 \\frac  {\\partial H_j }{\\partial \\theta_{3}}\n","\\end{pmatrix} =\n","\\begin{pmatrix} 0.2 \\\\0.2 \\\\0\\end{pmatrix} \\otimes \\theta + \\frac15 \\sum\\limits_{j=1}^5\\nabla_{\\theta} H_j = \\begin{pmatrix} 0.2 \\\\0.2 \\\\0\\end{pmatrix} \\otimes \\theta - \\frac15 \\sum\\limits_{j=1}^5 y_j \\cdot [M_j < 1] \\cdot x_{new,j} =\n","$$\n","\n","$$\n","= \\begin{pmatrix} 0.2 \\\\0.2 \\\\0\\end{pmatrix} \\otimes \\theta - \\frac15 \\sum\\limits_{j=1}^5 y_j \\cdot [y_j \\cdot \\langle x_{new,j}, \\theta \\rangle < 1] \\cdot x_{new,j}\n","$$\n","\n","Тут индекс $j$ опять же обозначает номер семпла; $\\otimes$ - поэлементное умножение векторов (операция Адамара, аналог операции * в numpy)."],"metadata":{"id":"a22XyVLGDI2O"},"id":"a22XyVLGDI2O"},{"cell_type":"code","execution_count":1,"id":"84be44bf","metadata":{"id":"84be44bf","executionInfo":{"status":"ok","timestamp":1692956572662,"user_tz":-180,"elapsed":8,"user":{"displayName":"Слава Яблочников","userId":"03313500950752150981"}}},"outputs":[],"source":["import numpy as np\n","import yaml"]},{"cell_type":"code","execution_count":2,"id":"8e381337","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e381337","outputId":"dbf0fde6-d5b2-49e2-9738-55b6f68c0be7","executionInfo":{"status":"ok","timestamp":1692956572663,"user_tz":-180,"elapsed":8,"user":{"displayName":"Слава Яблочников","userId":"03313500950752150981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction: [ 1.   2.   1.   0.  -0.5]\n","Loss: 0.5\n"]}],"source":["X = np.array([\n","    [0, 1],\n","    [1, 1],\n","    [1, 0],\n","    [-0.5, 0.5],\n","    [0, -0.5]\n","])\n","\n","y = np.array([1, 1, 1, -1, -1])\n","\n","theta0 = np.array([1.0, 1.0, 0.0])\n","\n","lr = 0.1\n","\n","def f(X, theta):\n","    theta = np.asarray(theta)\n","    return (X * theta[:2]).sum(axis=-1) + theta[2]\n","\n","def loss(X, y, theta):\n","    theta = np.asarray(theta)\n","    norm = (theta[:2] ** 2).sum()\n","    deltas = y * f(X, theta)\n","    return 0.1 * norm + np.mean(np.maximum(0, 1 - deltas))\n","\n","print(\"Prediction:\", f(X, theta0))\n","print(\"Loss:\", loss(X, y, theta0))"]},{"cell_type":"code","execution_count":3,"id":"ca79f1da","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca79f1da","executionInfo":{"status":"ok","timestamp":1692956572663,"user_tz":-180,"elapsed":7,"user":{"displayName":"Слава Яблочников","userId":"03313500950752150981"}},"outputId":"eea91a83-9481-4f51-f7e7-0127c52cbbdf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1.49999998,  1.        , -0.5       ])"]},"metadata":{},"execution_count":3}],"source":["# Ваш код оптимизации.\n","def grad_descent(X, y, theta0, lr=0.1):\n","    X_new = X.copy()\n","    X_new = np.concatenate((X_new, np.ones((X.shape[0], 1))), axis=1)\n","    a = np.array([0.2, 0.2, 0])\n","    theta = np.array(theta0)\n","\n","    for j in range(1000):\n","        I = X_new * (((y * (X_new @ theta)) < 1).reshape(-1, 1))\n","        grad =  -np.sum(I * (y / 5).reshape(-1, 1), axis=0)\n","        grad_full =  a * theta + grad\n","        theta -= lr * grad_full\n","\n","    return theta\n","\n","\n","theta = grad_descent(X, y, theta0, 0.1)\n","theta"]},{"cell_type":"code","execution_count":4,"id":"011ca5b7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"011ca5b7","executionInfo":{"status":"ok","timestamp":1692956572663,"user_tz":-180,"elapsed":5,"user":{"displayName":"Слава Яблочников","userId":"03313500950752150981"}},"outputId":"526f2d8b-1b61-4c1f-9787-88ef9d1305d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction: [ 0.5         1.99999998  0.99999998 -0.74999999 -1.        ]\n","Loss: 0.4750000000000001\n"]}],"source":["print(\"Prediction:\", f(X, theta))\n","print(\"Loss:\", loss(X, y, theta))\n","\n","with open(\"submission.yaml\", \"w\") as fp:\n","    yaml.safe_dump({\"tasks\": [{\"task1\": {\"answer\": theta.tolist()}}]}, fp)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}